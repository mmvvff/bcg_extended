{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a207638d-2d90-4a41-81e0-f86b35a174f9",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "21fa22a3-62fc-4ee9-ad3b-5e3fbc059bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from statsmodels.tools import categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805367dc",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "This section brings together all our user-defined functions we'll use throughout this document. These functions address three main areas:\n",
    "- Data Completeness and Missing Values: These help identify and handle missing data issues that might affect our analysis.\n",
    "- Data Formatting and Type Consistency: These ensure that our data is formatted consistently and uses the correct data types for accurate analysis.\n",
    "- Visualization Support: Inspired by the BCQ starter kit, these functions help create clear and informative visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "c2e7d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helper functions\n",
    "# get name of df\n",
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "82e4429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions 1. Data quality: A. Completeness and Missing values\n",
    "## Functions\n",
    "# replace unknown with np.nan\n",
    "def replace_unknown_object_cols(df: pd.DataFrame, na_values:list = [\"unknown\"]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replaces values \"unknown\" (or supplied values) with np.NaN in object and string-type columns of a DataFrame.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to modify.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the replaced values.\n",
    "    \"\"\"\n",
    "    object_cols = df.select_dtypes(include=[object,\"string\"]).columns\n",
    "    default_value = [\"unknown\"]\n",
    "    if na_values != default_value:\n",
    "        na_values = na_values + default_value\n",
    "    na_values_lower = [x.lower() for x in na_values]\n",
    "    pattern = \"|\".join(na_values_lower)\n",
    "    for col in object_cols:\n",
    "        df[col] = df[col].str.strip().str.lower()\n",
    "        df[col] = df[col].replace(pattern, np.nan, regex=True)\n",
    "    return df\n",
    "\n",
    "# assess prevalence of missingness\n",
    "def prevalence_missingness(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of missing values per column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A Series with column names as the index and the percentage of missing values as values.\n",
    "    \"\"\"\n",
    "    missing_percentage = round(df.isnull().sum() / len(df), 4) * 100\n",
    "    missing_percentage = missing_percentage.reset_index().copy()\n",
    "    missing_percentage = missing_percentage.rename(columns={\"index\":\"col_name\",0:\"prevalence_na\"})\n",
    "    return missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "06961b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions 2. Data quality: B. Consistency in data formatting\n",
    "## Functions\n",
    "\n",
    "# convert to bool those columns whose values are [0,1] or [f,t]\n",
    "def convert_bool_cols(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts columns in a DataFrame to bool if their unique values are [0, 1, np.nan].\n",
    "    Args:\n",
    "        df: A pandas DataFrame.\n",
    "    Returns:\n",
    "        The DataFrame with qualifying columns converted to bool.\n",
    "    \"\"\"\n",
    "    # for [f,t]\n",
    "    object_cols = df.select_dtypes(include=[object,\"string\"]).columns\n",
    "    for col in df[object_cols].columns:\n",
    "        df[col] = df.loc[:,col].str.lower().copy()\n",
    "        if set(df[col].unique()) <= {\"f\",\"t\", np.nan}:\n",
    "            df[col] = df.loc[:,col].map({\"f\": 0, \"t\": 1}).copy()\n",
    "    # for [0,1]\n",
    "    for col in df.columns:\n",
    "        if set(df[col].unique()) <= {0, 1, np.nan}:\n",
    "            df[col] = df[col].astype(bool).copy()\n",
    "    return df\n",
    "\n",
    "# convert string columns matching 'date' to pd.datetime\n",
    "def convert_obj_to_date(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts object columns containing dates to datetime format in a pandas DataFrame.\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The DataFrame to convert.\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame with object columns converted to datetime.\n",
    "    \"\"\"\n",
    "    cols_date = [col for col in df.columns if 'date' in col]\n",
    "    for col in df[cols_date].select_dtypes(include=[object,\"string\"]):\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col]).copy()\n",
    "        except:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "# convert string columns to category\n",
    "def convert_to_category(df:pd.DataFrame, nunique_cutoff:int = 12) -> pd.DataFrame:\n",
    "    \"\"\"Converts string columns to pandas category if the column has less than 10 unique values.\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The DataFrame to convert.\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame with string columns converted to category if applicable.\n",
    "    \"\"\"\n",
    "    for col in df.select_dtypes(include=[object,\"string\"]):\n",
    "        if df[col].nunique() <= nunique_cutoff:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "561e902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function 3. Feature engineer\n",
    "\n",
    "# function that avoids repeated column names when joining aggregated dataframes\n",
    "def append_suffix(df:pd.DataFrame, columns:list, suffix:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Appends a suffix to the names of selected columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to modify.\n",
    "        columns (list): A list of column names to modify.\n",
    "        suffix (str): The suffix to append.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame with modified column names.\n",
    "    \"\"\"\n",
    "\n",
    "    new_cols = [col + suffix for col in columns if col in df.columns]\n",
    "    return df.rename(columns=dict(zip(columns, new_cols)))\n",
    "\n",
    "# Function to calculate the correlation coefficients for the lower off-diagonal elements\n",
    "def correlation_matrix(df:pd.DataFrame, numeric_cols:list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the correlation matrix for a subset of numeric columns in a DataFrame.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to analyze.\n",
    "        cols (list): A list of column names for which to calculate correlations.\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the correlation coefficients.\n",
    "    \"\"\"\n",
    "    # Create a correlation matrix for the numeric columns\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    # # Extract column names for the upper triangle of the correlation matrix (excluding redundant correlations)\n",
    "    upper_tri_cols = [(col1, col2) for col1 in corr_matrix.columns for col2 in corr_matrix.columns if col1 > col2]\n",
    "    # Create a DataFrame to store correlation coefficients\n",
    "    corr_df = pd.DataFrame(columns=[\"name_col1\", \"name_col2\", \"correlation\"])\n",
    "    # Fill the DataFrame with correlation coefficients and column names\n",
    "    for i, (col1, col2) in enumerate(upper_tri_cols):\n",
    "        corr_df.loc[i] = [col1, col2, corr_matrix.loc[col1, col2]]\n",
    "    corr_df = corr_df.loc[corr_df[\"correlation\"] < 1].copy()\n",
    "    return corr_df\n",
    "\n",
    "# Function that reduces a set of numerical columns to its most important PC\n",
    "def extract_principal_components(df, cols):\n",
    "    \"\"\"\n",
    "    Extracts principal components from a DataFrame using selected columns.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the data.\n",
    "        cols (list): A list of column names to use for PCA.\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the principal components.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scale the data\n",
    "    scaled_data = scale(df[cols])\n",
    "    # Create a PCA object\n",
    "    pca = PCA()\n",
    "    # Fit the PCA model to the scaled data\n",
    "    pca.fit(scaled_data)\n",
    "    # Transform the scaled data to principal components\n",
    "    principal_components = pca.transform(scaled_data)\n",
    "    # Create a DataFrame to store the principal components\n",
    "    pc_df = pd.DataFrame(principal_components, columns=[f\"pc_{i+1}\" for i in range(len(cols))])\n",
    "    # choose number of components that explain 95% of variance \n",
    "    pc_explained_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components_95 = np.argmax(pc_explained_var >= 0.95) + 1\n",
    "    # Reduce the number of components to those that explain 95% of the variance\n",
    "    pc_df = pc_df.iloc[:, :n_components_95]\n",
    "    return pc_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb5312-cbc0-4193-b84e-2c94455c9edf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Load\n",
    "We load the data with `convert_types()` to ensure precise data type definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "82e6a4ae-2b8a-4152-b04f-0c998466fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df = pd.read_csv('data/clean_data_after_eda.csv').convert_dtypes()\n",
    "price_df = pd.read_csv('data/price_data.csv').convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7aaab-7c26-40ca-8f63-cf2ef9541ce6",
   "metadata": {},
   "source": [
    "We use `info()` and `head()` to get a basic understanding of our two main datasets: `client_df` and `price_df`.\n",
    "\n",
    "**Main Takeaways:**\n",
    "\n",
    "* **Size:** `client_df` has 43 columns and 14606 rows, while `price_df` has 8 columns and 193002 rows.\n",
    "\n",
    "* **Content:** `client_df` holds client characteristics, while `price_df` is a time series of prices.\n",
    "\n",
    "* **Connection:** Both data frames share a primary key named `id`.\n",
    "\n",
    "* **Missing Values:** While initial inspection suggests no missing values (non-null count equals entries), further analysis might reveal imputed values.\n",
    "\n",
    "* **Date Format:** Date columns in both data frames are currently strings and need conversion to datetime format.\n",
    "\n",
    "* **Data Types:** `price_df` contains mostly numerical data (besides date data and `id`), while `client_df` has a combination of numerical and categorical data.\n",
    "\n",
    "* **Categorical Variables in `client_df`:**\n",
    "    * **Dichotomous:** Variables like `churn` and `has_gas` are currently stored as integer and string, respectively. We'll convert them to booleans to improve readability and consistency.\n",
    "    * **Nominal:** Columns like `channel_sales` and `origin_up` have nominal categorical data. We'll convert them to the `categorical` type for better handling and readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "dcf0d488-d3df-4819-a52b-e335c0674e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14606 entries, 0 to 14605\n",
      "Data columns (total 44 columns):\n",
      " #   Column                          Dtype  \n",
      "---  ------                          -----  \n",
      " 0   id                              string \n",
      " 1   channel_sales                   string \n",
      " 2   cons_12m                        Int64  \n",
      " 3   cons_gas_12m                    Int64  \n",
      " 4   cons_last_month                 Int64  \n",
      " 5   date_activ                      string \n",
      " 6   date_end                        string \n",
      " 7   date_modif_prod                 string \n",
      " 8   date_renewal                    string \n",
      " 9   forecast_cons_12m               Float64\n",
      " 10  forecast_cons_year              Int64  \n",
      " 11  forecast_discount_energy        Int64  \n",
      " 12  forecast_meter_rent_12m         Float64\n",
      " 13  forecast_price_energy_off_peak  Float64\n",
      " 14  forecast_price_energy_peak      Float64\n",
      " 15  forecast_price_pow_off_peak     Float64\n",
      " 16  has_gas                         string \n",
      " 17  imp_cons                        Float64\n",
      " 18  margin_gross_pow_ele            Float64\n",
      " 19  margin_net_pow_ele              Float64\n",
      " 20  nb_prod_act                     Int64  \n",
      " 21  net_margin                      Float64\n",
      " 22  num_years_antig                 Int64  \n",
      " 23  origin_up                       string \n",
      " 24  pow_max                         Float64\n",
      " 25  var_year_price_off_peak_var     Float64\n",
      " 26  var_year_price_peak_var         Float64\n",
      " 27  var_year_price_mid_peak_var     Float64\n",
      " 28  var_year_price_off_peak_fix     Float64\n",
      " 29  var_year_price_peak_fix         Float64\n",
      " 30  var_year_price_mid_peak_fix     Float64\n",
      " 31  var_year_price_off_peak         Float64\n",
      " 32  var_year_price_peak             Float64\n",
      " 33  var_year_price_mid_peak         Float64\n",
      " 34  var_6m_price_off_peak_var       Float64\n",
      " 35  var_6m_price_peak_var           Float64\n",
      " 36  var_6m_price_mid_peak_var       Float64\n",
      " 37  var_6m_price_off_peak_fix       Float64\n",
      " 38  var_6m_price_peak_fix           Float64\n",
      " 39  var_6m_price_mid_peak_fix       Float64\n",
      " 40  var_6m_price_off_peak           Float64\n",
      " 41  var_6m_price_peak               Float64\n",
      " 42  var_6m_price_mid_peak           Float64\n",
      " 43  churn                           Int64  \n",
      "dtypes: Float64(28), Int64(8), string(8)\n",
      "memory usage: 5.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>channel_sales</th>\n",
       "      <th>cons_12m</th>\n",
       "      <th>cons_gas_12m</th>\n",
       "      <th>cons_last_month</th>\n",
       "      <th>date_activ</th>\n",
       "      <th>date_end</th>\n",
       "      <th>date_modif_prod</th>\n",
       "      <th>date_renewal</th>\n",
       "      <th>forecast_cons_12m</th>\n",
       "      <th>...</th>\n",
       "      <th>var_6m_price_off_peak_var</th>\n",
       "      <th>var_6m_price_peak_var</th>\n",
       "      <th>var_6m_price_mid_peak_var</th>\n",
       "      <th>var_6m_price_off_peak_fix</th>\n",
       "      <th>var_6m_price_peak_fix</th>\n",
       "      <th>var_6m_price_mid_peak_fix</th>\n",
       "      <th>var_6m_price_off_peak</th>\n",
       "      <th>var_6m_price_peak</th>\n",
       "      <th>var_6m_price_mid_peak</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24011ae4ebbe3035111d65fa7c15bc57</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>0</td>\n",
       "      <td>54946</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-15</td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>2.086294</td>\n",
       "      <td>99.530517</td>\n",
       "      <td>44.235794</td>\n",
       "      <td>2.086425</td>\n",
       "      <td>99.530558</td>\n",
       "      <td>44.236702</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d29c2c54acc38ff3c0614d0a653813dd</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>4660</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2016-08-30</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>189.95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>764c75f661154dac3a6c254cd082ea7d</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2015-04-17</td>\n",
       "      <td>47.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                     channel_sales  \\\n",
       "0  24011ae4ebbe3035111d65fa7c15bc57  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "1  d29c2c54acc38ff3c0614d0a653813dd                           MISSING   \n",
       "2  764c75f661154dac3a6c254cd082ea7d  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "\n",
       "   cons_12m  cons_gas_12m  cons_last_month  date_activ    date_end  \\\n",
       "0         0         54946                0  2013-06-15  2016-06-15   \n",
       "1      4660             0                0  2009-08-21  2016-08-30   \n",
       "2       544             0                0  2010-04-16  2016-04-16   \n",
       "\n",
       "  date_modif_prod date_renewal  forecast_cons_12m  ...  \\\n",
       "0      2015-11-01   2015-06-23                0.0  ...   \n",
       "1      2009-08-21   2015-08-31             189.95  ...   \n",
       "2      2010-04-16   2015-04-17              47.96  ...   \n",
       "\n",
       "   var_6m_price_off_peak_var  var_6m_price_peak_var  \\\n",
       "0                   0.000131               0.000041   \n",
       "1                   0.000003               0.001218   \n",
       "2                   0.000004                    0.0   \n",
       "\n",
       "   var_6m_price_mid_peak_var  var_6m_price_off_peak_fix  \\\n",
       "0                   0.000908                   2.086294   \n",
       "1                        0.0                   0.009482   \n",
       "2                        0.0                        0.0   \n",
       "\n",
       "   var_6m_price_peak_fix  var_6m_price_mid_peak_fix var_6m_price_off_peak  \\\n",
       "0              99.530517                  44.235794              2.086425   \n",
       "1                    0.0                        0.0              0.009485   \n",
       "2                    0.0                        0.0              0.000004   \n",
       "\n",
       "   var_6m_price_peak  var_6m_price_mid_peak  churn  \n",
       "0          99.530558              44.236702      1  \n",
       "1           0.001218                    0.0      0  \n",
       "2                0.0                    0.0      0  \n",
       "\n",
       "[3 rows x 44 columns]"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_df.info(verbose=True, max_cols=30)\n",
    "client_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "d7861206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193002 entries, 0 to 193001\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   id                  193002 non-null  string \n",
      " 1   price_date          193002 non-null  string \n",
      " 2   price_off_peak_var  193002 non-null  Float64\n",
      " 3   price_peak_var      193002 non-null  Float64\n",
      " 4   price_mid_peak_var  193002 non-null  Float64\n",
      " 5   price_off_peak_fix  193002 non-null  Float64\n",
      " 6   price_peak_fix      193002 non-null  Float64\n",
      " 7   price_mid_peak_fix  193002 non-null  Float64\n",
      "dtypes: Float64(6), string(2)\n",
      "memory usage: 12.9 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_date</th>\n",
       "      <th>price_off_peak_var</th>\n",
       "      <th>price_peak_var</th>\n",
       "      <th>price_mid_peak_var</th>\n",
       "      <th>price_off_peak_fix</th>\n",
       "      <th>price_peak_fix</th>\n",
       "      <th>price_mid_peak_fix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>038af19179925da21a25619c5a24b745</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.266931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>038af19179925da21a25619c5a24b745</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.266931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>038af19179925da21a25619c5a24b745</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.266931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  price_date  price_off_peak_var  \\\n",
       "0  038af19179925da21a25619c5a24b745  2015-01-01            0.151367   \n",
       "1  038af19179925da21a25619c5a24b745  2015-02-01            0.151367   \n",
       "2  038af19179925da21a25619c5a24b745  2015-03-01            0.151367   \n",
       "\n",
       "   price_peak_var  price_mid_peak_var  price_off_peak_fix  price_peak_fix  \\\n",
       "0             0.0                 0.0           44.266931             0.0   \n",
       "1             0.0                 0.0           44.266931             0.0   \n",
       "2             0.0                 0.0           44.266931             0.0   \n",
       "\n",
       "   price_mid_peak_fix  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  "
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_df.info()\n",
    "price_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71120a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Data cleaning and preparation\n",
    "\n",
    "### Data quality: Completeness and Missing values\n",
    "\n",
    "This section focuses on finding and handling missing values in the categorical columns.\n",
    "\n",
    "We use the `channel_sales` column as our starting point. The output of `head()` below shows that it has a value of `\"MISSING\"` to indicate missing data (NA). We'll check if other categorical columns use similar values (e.g., \"NA\", \"Unknown\") to represent NA.\n",
    "\n",
    "Since categorical data in these columns is stored as hashed text strings (refer to the data's documentation), they usually have a fixed length. We leverage this characteristic to identify missing values. We compare the length of each string to the maximum length within the column. Entries with a shorter length might indicate missing values that have been hashed with a special value. In contrast, we consider that if all values share the same length, NA are absent.\n",
    "\n",
    "All values identified correspond to 'MISSING'. Therefore, we replace these values with `np.nan` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "075ed728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>channel_sales</th>\n",
       "      <th>date_activ</th>\n",
       "      <th>date_end</th>\n",
       "      <th>date_modif_prod</th>\n",
       "      <th>date_renewal</th>\n",
       "      <th>has_gas</th>\n",
       "      <th>origin_up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24011ae4ebbe3035111d65fa7c15bc57</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>2013-06-15</td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>2015-06-23</td>\n",
       "      <td>t</td>\n",
       "      <td>lxidpiddsbxsbosboudacockeimpuepw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d29c2c54acc38ff3c0614d0a653813dd</td>\n",
       "      <td>MISSING</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2016-08-30</td>\n",
       "      <td>2009-08-21</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>f</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>764c75f661154dac3a6c254cd082ea7d</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>2015-04-17</td>\n",
       "      <td>f</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                     channel_sales  \\\n",
       "0  24011ae4ebbe3035111d65fa7c15bc57  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "1  d29c2c54acc38ff3c0614d0a653813dd                           MISSING   \n",
       "2  764c75f661154dac3a6c254cd082ea7d  foosdfpfkusacimwkcsosbicdxkicaua   \n",
       "\n",
       "   date_activ    date_end date_modif_prod date_renewal has_gas  \\\n",
       "0  2013-06-15  2016-06-15      2015-11-01   2015-06-23       t   \n",
       "1  2009-08-21  2016-08-30      2009-08-21   2015-08-31       f   \n",
       "2  2010-04-16  2016-04-16      2010-04-16   2015-04-17       f   \n",
       "\n",
       "                          origin_up  \n",
       "0  lxidpiddsbxsbosboudacockeimpuepw  \n",
       "1  kamkkxfxxuwbdslkwifmmcsiusiuosws  \n",
       "2  kamkkxfxxuwbdslkwifmmcsiusiuosws  "
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_df[client_df.select_dtypes(include=['string']).columns].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c448c",
   "metadata": {},
   "source": [
    "**First**, we iterate through all string columns in both dataframes to identify inconsistencies in string lengths and discover which values represent missing data (NA) in these columns.\n",
    "\n",
    "Our assessment reveals that only `client_df` has missing values, while `price_df` is complete. In both the `channel_sales` and `origin_up` columns within `client_df`, the value `\"MISSING\"` is used to indicate missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "660fae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_df:channel_sales has inconsistent string length: max 32, min 7\n",
      "client_df:origin_up has inconsistent string length: max 32, min 7\n",
      "client_df:channel_sales, strings of min length: ['MISSING']\n",
      "client_df:origin_up, strings of min length: ['MISSING']\n",
      "price_df has no inconsistent columns\n",
      "\n",
      "Unique values denoting NA in all string columns of all dfs: ['MISSING']\n"
     ]
    }
   ],
   "source": [
    "cols_string_missing_values = []\n",
    "for df in (client_df, price_df):\n",
    "    cols_string_inconsistent = []\n",
    "    for col in df.select_dtypes(include=['string']).columns:\n",
    "        if max(df[col].str.len()) != min(df[col].str.len()):\n",
    "            cols_string_inconsistent.append(col)\n",
    "        else:\n",
    "            continue\n",
    "    if cols_string_inconsistent:\n",
    "        for col in cols_string_inconsistent:\n",
    "            print(f\"{get_df_name(df)}:{col} has inconsistent string length: max {max(df[col].str.len())}, min {min(df[col].str.len())}\")\n",
    "    else:\n",
    "        print(f\"{get_df_name(df)} has no inconsistent columns\")\n",
    "\n",
    "    for col in cols_string_inconsistent:\n",
    "        col_min = min(df[col].str.len())\n",
    "        client_subset_loop = df[df[col].str.len() == col_min]\n",
    "        cols_string_missing_values.extend(list(client_subset_loop[col].unique()))\n",
    "        print(f\"{get_df_name(df)}:{col}, strings of min length: {list(client_subset_loop[col].unique())}\")\n",
    "print(f\"\\nUnique values denoting NA in all string columns of all dfs: {list(set(cols_string_missing_values))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b033f78e",
   "metadata": {},
   "source": [
    "**Second**, we address missing values. We use our custom `replace_unknown_object_cols()` function to replace the string 'MISSING' with `np.nan` in the previously identified columns. This step ensures a consistent representation of missing data across the dataset. We deliberately choose not to retain 'MISSING' as a separate category in these columns. Keeping it would effectively create an artificial attribute unrelated to the actual information these columns are meant to convey, potentially introducing bias into our model. We discuss below how we deal with these missing values.\n",
    "\n",
    "**Third**, we assess missingness prevalence. We use our custom function `prevalence_missingness()` to analyze the percentage of missing values in each column of both dataframes. The analysis reveals a high proportion of missing values (around 25%) in the `channel_sales` column, while the `origin_up` column has very few missing values (less than 1%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "17f9b8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>prevalence_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>channel_sales</td>\n",
       "      <td>25.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cons_12m</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cons_gas_12m</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cons_last_month</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>date_activ</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>date_end</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>date_modif_prod</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>date_renewal</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>forecast_cons_12m</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>forecast_cons_year</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>forecast_discount_energy</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>forecast_meter_rent_12m</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>forecast_price_energy_off_peak</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>forecast_price_energy_peak</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>forecast_price_pow_off_peak</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>has_gas</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>imp_cons</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>margin_gross_pow_ele</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>margin_net_pow_ele</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nb_prod_act</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>net_margin</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>num_years_antig</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>origin_up</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pow_max</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>var_year_price_off_peak_var</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>var_year_price_peak_var</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>var_year_price_mid_peak_var</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>var_year_price_off_peak_fix</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>var_year_price_peak_fix</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>var_year_price_mid_peak_fix</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>var_year_price_off_peak</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>var_year_price_peak</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>var_year_price_mid_peak</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>var_6m_price_off_peak_var</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>var_6m_price_peak_var</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>var_6m_price_mid_peak_var</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>var_6m_price_off_peak_fix</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>var_6m_price_peak_fix</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>var_6m_price_mid_peak_fix</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>var_6m_price_off_peak</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>var_6m_price_peak</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>var_6m_price_mid_peak</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>churn</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          col_name  prevalence_na\n",
       "0                               id           0.00\n",
       "1                    channel_sales          25.50\n",
       "2                         cons_12m           0.00\n",
       "3                     cons_gas_12m           0.00\n",
       "4                  cons_last_month           0.00\n",
       "5                       date_activ           0.00\n",
       "6                         date_end           0.00\n",
       "7                  date_modif_prod           0.00\n",
       "8                     date_renewal           0.00\n",
       "9                forecast_cons_12m           0.00\n",
       "10              forecast_cons_year           0.00\n",
       "11        forecast_discount_energy           0.00\n",
       "12         forecast_meter_rent_12m           0.00\n",
       "13  forecast_price_energy_off_peak           0.00\n",
       "14      forecast_price_energy_peak           0.00\n",
       "15     forecast_price_pow_off_peak           0.00\n",
       "16                         has_gas           0.00\n",
       "17                        imp_cons           0.00\n",
       "18            margin_gross_pow_ele           0.00\n",
       "19              margin_net_pow_ele           0.00\n",
       "20                     nb_prod_act           0.00\n",
       "21                      net_margin           0.00\n",
       "22                 num_years_antig           0.00\n",
       "23                       origin_up           0.44\n",
       "24                         pow_max           0.00\n",
       "25     var_year_price_off_peak_var           0.00\n",
       "26         var_year_price_peak_var           0.00\n",
       "27     var_year_price_mid_peak_var           0.00\n",
       "28     var_year_price_off_peak_fix           0.00\n",
       "29         var_year_price_peak_fix           0.00\n",
       "30     var_year_price_mid_peak_fix           0.00\n",
       "31         var_year_price_off_peak           0.00\n",
       "32             var_year_price_peak           0.00\n",
       "33         var_year_price_mid_peak           0.00\n",
       "34       var_6m_price_off_peak_var           0.00\n",
       "35           var_6m_price_peak_var           0.00\n",
       "36       var_6m_price_mid_peak_var           0.00\n",
       "37       var_6m_price_off_peak_fix           0.00\n",
       "38           var_6m_price_peak_fix           0.00\n",
       "39       var_6m_price_mid_peak_fix           0.00\n",
       "40           var_6m_price_off_peak           0.00\n",
       "41               var_6m_price_peak           0.00\n",
       "42           var_6m_price_mid_peak           0.00\n",
       "43                           churn           0.00"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_df = replace_unknown_object_cols(client_df, cols_string_missing_values).copy()\n",
    "prevalence_missingness(client_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5353f99",
   "metadata": {},
   "source": [
    "**Fourth**, we address the relative high prevalence of NA in the `channel_sales` column.  We consider two approaches to tackle this issue:\n",
    "\n",
    "   - **Option 1: Drop the Column:** This approach prioritizes keeping all observations in the dataset (maximizing sample size) even if it means removing the `channel_sales` column entirely.\n",
    "   - **Option 2: Drop Observations with Missing Values:** This approach prioritizes keeping the `channel_sales` column (preserving the information it provides) even if it means removing some observations from the dataset (reducing sample size).\n",
    "\n",
    "We choose the second option for the following two reasons:\n",
    "\n",
    "   - The relatively large initial sample size of the `client_df` dataframe (14,606 rows) allows for some reduction without significantly impacting analysis.\n",
    "   - The `channel_sales` column offers valuable information: \n",
    "      - It contains categorical nominal data, which is uncommon in our datasets.\n",
    "      - It directly relates to our core analytical focus: defensive marketing and churn prevention.\n",
    "\n",
    "Therefore, we opt to remove observations with missing values in the `channel_sales` or in `origin_up` columns to retain these crucial data points for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "53d2e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NAs\n",
    "client_df = client_df.dropna().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a56c0",
   "metadata": {},
   "source": [
    "### Data quality: Formatting and Type Consistency\n",
    "\n",
    "First, we addresses data type conversion to enhance data handling, consistency, and readability. We proceed as follows:\n",
    "\n",
    "* Categorical dichotomous data (stored as integers or strings) to Boolean.\n",
    "* Date data (stored as strings) to datetime.\n",
    "* Categorical nominal data (stored as strings) to category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "2319c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to bool\n",
    "client_df = convert_bool_cols(client_df).copy()\n",
    "price_df = convert_bool_cols(price_df).copy()\n",
    "\n",
    "# convert to pd.datetime\n",
    "client_df = convert_obj_to_date(client_df).copy()\n",
    "price_df = convert_obj_to_date(price_df).copy()\n",
    "\n",
    "# convert to categorical data\n",
    "client_df = convert_to_category(client_df).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f1904",
   "metadata": {},
   "source": [
    "Second, we identify and address data inconsistencies arising in our categorical and date data.\n",
    "\n",
    "First, we address inconsistencies in the column `has_gas`, which indicates whether a customer is a gas client. We assume that a customer with positive gas consumption in the past 12 months (reflected in the `cons_gas_12m field` > 0) should be marked as a gas client (`has_gas` = `True`). We identify and correct any discrepancies based on this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "2b8c25ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of inconsistencies in has_gas: 41\n"
     ]
    }
   ],
   "source": [
    "# assess inconsistencies in data\n",
    "# we drop those cases where \"has_gas\" and \"cons_gas_12m\" are inconsistent\n",
    "client_df[\"gas_inconsistent\"] = [\n",
    "    True if (a > 0 and not b) else False \n",
    "    for a, b in zip(client_df[\"cons_gas_12m\"], client_df[\"has_gas\"])\n",
    "]\n",
    "size_gas = len(client_df[client_df[\"gas_inconsistent\"]==True])\n",
    "if size_gas > 0:\n",
    "    print(f\"Size of inconsistencies in has_gas: {size_gas}\")\n",
    "\n",
    "    # we replace those case that are inconsistent with \"True\",\n",
    "    # otherwise if leave them as is\n",
    "    client_df[\"has_gas\"] = np.where(client_df[\"gas_inconsistent\"], True, client_df[\"has_gas\"]).copy()\n",
    "    # we may also remove these is necessary\n",
    "    # client_df = client_df.drop(client_df[client_df[\"gas_inconsistent\"]==True].index).copy()\n",
    "client_df = client_df.drop(columns=[\"gas_inconsistent\"]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818ec66",
   "metadata": {},
   "source": [
    "**Second,** we examine our date data for inconsistencies. We evaluate the following three assumptions:\n",
    "\n",
    "* **Assumption 1:** `date_activ` (date of contract activation) should be earlier than any other date values, such as `date_end` (registered date of contract termination) or `date_renewal` (date of the next contract renewal). If this assumption is violated, we drop these observations.\n",
    "* **Assumption 2:** `date_end` (registered date of contract termination) should be the latest date per row. In other words, all other dates related to contract status should be earlier. Inconsistencies result in dropping the observations.\n",
    "* **Assumption 3:** `num_years_antig` should be equal to `date_end` minus `date_activ`. If this assumption is not met, we estimate and replace the inconsistent values in `num_years_antig`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "298e4a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assumption 1: date_active is the lowest value per row\n",
      "Size of inconsistencies across rows in date_activ: 20\n",
      "Inconsistent cases dropped in date_activ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dates\n",
    "cols_client_date = [col for col in client_df.columns if 'date' in col]\n",
    "\n",
    "# assumption 1: date_active is the lowest value per row\n",
    "print(\"assumption 1: date_active is the lowest value per row\")\n",
    "client_df[\"date_min\"] = client_df[cols_client_date].min(axis=1)\n",
    "client_df[\"date_activ_inconsistent\"] = [\n",
    "    True if (a > b) else False \n",
    "    for a, b in zip(client_df[\"date_activ\"], client_df[\"date_min\"])\n",
    "]\n",
    "size_1 = len(client_df[client_df[\"date_activ_inconsistent\"]==True])\n",
    "if size_1 > 0:\n",
    "    print(f\"Size of inconsistencies across rows in date_activ: {size_1}\")\n",
    "    client_df = client_df.drop(client_df[client_df[\"date_activ_inconsistent\"]==True].index).copy()\n",
    "    print(f\"Inconsistent cases dropped in date_activ\\n\")\n",
    "else:\n",
    "    print(f\"There are no inconsistencies across rows in date_activ\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "4be4a7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assumption 2: date_end is the highest value per row\n",
      "There are no inconsistencies across rows in date_end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assumption 2: date_end is the highest value per row\n",
    "print(\"assumption 2: date_end is the highest value per row\")\n",
    "client_df[\"date_max\"] = client_df[cols_client_date].max(axis=1)\n",
    "client_df[\"date_end_inconsistent\"] = [\n",
    "    True if (a < b) else False \n",
    "    for a, b in zip(client_df[\"date_end\"], client_df[\"date_max\"])\n",
    "]\n",
    "size_2 = len(client_df[client_df[\"date_end_inconsistent\"]==True])\n",
    "if size_2 > 0:\n",
    "    print(f\"Size of inconsistencies across rows in date_end: {size_2}\")\n",
    "    client_df = client_df.drop(client_df[client_df[\"date_end_inconsistent\"]==True].index).copy()\n",
    "    print(f\"Inconsistent cases dropped in date_end\\n\")\n",
    "else:\n",
    "    print(f\"There are no inconsistencies across rows in date_end\\n\")\n",
    "client_df = client_df.drop(columns=[\"date_min\",\"date_activ_inconsistent\",\"date_max\",\"date_end_inconsistent\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "d89cc15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assumption 3: num_years_antig is equivalent to date_end - date_activ\n",
      "Size of inconsistencies across rows in num_years_antig: 4863\n",
      "Min inconsistency per row: 2 years\n",
      "Max inconsistency per row: 11 years\n",
      "Inconsistent cases replaced with true estimates in num_years_antig\n"
     ]
    }
   ],
   "source": [
    "# assumption 3: num_years_antig is equivalent to date_end - date_activ\n",
    "print(\"assumption 3: num_years_antig is equivalent to date_end - date_activ\")\n",
    "client_df[\"num_years_check\"] = client_df[\"date_end\"].dt.year - client_df[\"date_activ\"].dt.year\n",
    "client_df[\"num_years_inconsistent\"] = [\n",
    "    True if (a != b) else False \n",
    "    for a, b in zip(client_df[\"num_years_antig\"], client_df[\"num_years_check\"])\n",
    "]\n",
    "size_3 = len(client_df[client_df[\"num_years_inconsistent\"]==True])\n",
    "min_3 = client_df[\"num_years_check\"].min()\n",
    "max_3 = client_df[\"num_years_check\"].max()\n",
    "if size_3 > 0:\n",
    "    print(f\"Size of inconsistencies across rows in num_years_antig: {size_3}\")\n",
    "    print(f\"Min inconsistency per row: {min_3} years\")\n",
    "    print(f\"Max inconsistency per row: {max_3} years\")\n",
    "    \n",
    "    # we replace values with our estimates\n",
    "    client_df[\"num_years_antig\"] = client_df[\"num_years_check\"].astype(\"Int64\")\n",
    "    print(f\"Inconsistent cases replaced with true estimates in num_years_antig\")\n",
    "else:\n",
    "    print(f\"There are no inconsistencies across rows in num_years_antig\")\n",
    "client_df = client_df.drop(columns=[\"num_years_check\",\"num_years_inconsistent\"]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d717c",
   "metadata": {},
   "source": [
    "### Data quality: Duplicates\n",
    "\n",
    "Lastly, we assess duplicated entries. We do not find any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "e794f1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of duplicates in client_df: 0\n",
      "Count of duplicates in price_df: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of duplicates in client_df:\",client_df.duplicated(keep='first').sum())\n",
    "print(\"Count of duplicates in price_df:\",price_df.duplicated(keep='first').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9e960",
   "metadata": {},
   "source": [
    "### The cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "72c44df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of client_df: (10811, 44)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Float64           28\n",
       "Int64              7\n",
       "datetime64[ns]     4\n",
       "bool               2\n",
       "string[python]     1\n",
       "category           1\n",
       "category           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dimensions of client_df:\",client_df.shape)\n",
    "client_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "0db2b6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of client_df: (193002, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Float64           6\n",
       "string[python]    1\n",
       "datetime64[ns]    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dimensions of client_df:\",price_df.shape)\n",
    "price_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a9e80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature engineering\n",
    "\n",
    "As noted earlier, our working hypothesis is that price increases affect customer churn. First, we focus on the development of variables related to prices.\n",
    "\n",
    "### Prices\n",
    "\n",
    "First, we develop new variables based on `price_df`. To facilitate readability, we replace the suffixes with appropriate names, according to the data documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "a6c639d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193002 entries, 0 to 193001\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   id                     193002 non-null  string        \n",
      " 1   price_date             193002 non-null  datetime64[ns]\n",
      " 2   price_off_peak_energy  193002 non-null  Float64       \n",
      " 3   price_peak_energy      193002 non-null  Float64       \n",
      " 4   price_mid_peak_energy  193002 non-null  Float64       \n",
      " 5   price_off_peak_power   193002 non-null  Float64       \n",
      " 6   price_peak_power       193002 non-null  Float64       \n",
      " 7   price_mid_peak_power   193002 non-null  Float64       \n",
      "dtypes: Float64(6), datetime64[ns](1), string(1)\n",
      "memory usage: 12.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Function to replace suffixes in column names\n",
    "def replace_suffixes(col_name):\n",
    "    if '_var' in col_name:\n",
    "        return col_name.replace('_var', '_energy')\n",
    "    elif '_fix' in col_name:\n",
    "        return col_name.replace('_fix', '_power')\n",
    "    else:\n",
    "        return col_name\n",
    "\n",
    "# Applying the function to each column name\n",
    "price_df.columns = [replace_suffixes(col) for col in price_df.columns]\n",
    "\n",
    "price_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24932427",
   "metadata": {},
   "source": [
    "All dates correspond the 1st of each month. Therefore, each row of data is equivalent to the monthly price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "16353348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          price_date\n",
      "count                         193002\n",
      "mean   2015-06-16 12:50:49.933161216\n",
      "min              2015-01-01 00:00:00\n",
      "25%              2015-04-01 00:00:00\n",
      "50%              2015-07-01 00:00:00\n",
      "75%              2015-10-01 00:00:00\n",
      "max              2015-12-01 00:00:00\n",
      "2015-01-01 00:00:00\n",
      "2015-02-01 00:00:00\n",
      "2015-03-01 00:00:00\n",
      "2015-04-01 00:00:00\n",
      "2015-05-01 00:00:00\n",
      "2015-06-01 00:00:00\n",
      "2015-07-01 00:00:00\n",
      "2015-08-01 00:00:00\n",
      "2015-09-01 00:00:00\n",
      "2015-10-01 00:00:00\n",
      "2015-11-01 00:00:00\n",
      "2015-12-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# descriptive statistics only for datetime columns\n",
    "print(price_df.describe(include=[\"datetime\"]))\n",
    "# the unique values in datetime column\n",
    "print(*price_df[\"price_date\"].unique().tolist(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9414634",
   "metadata": {},
   "source": [
    "To make it easier to create new price-based features and group data for calculations, we prepare the data by extracting the year and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "4e5cdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month as separate columns from the 'price_date' column\n",
    "price_df[\"price_year\"] = price_df[\"price_date\"].dt.year.astype(\"category\")  # Year as categorical variable\n",
    "price_df[\"price_month\"] = price_df[\"price_date\"].dt.month.astype(\"category\")  # Month as categorical variable\n",
    "\n",
    "# Reorder columns to group 'id', 'price_year', 'price_month' at the front while preserving the order of other columns\n",
    "price_df = price_df[[\"id\",\"price_year\",\"price_month\"] + [c for c in price_df if c not in [\"id\",\"price_year\",\"price_month\"]]]\n",
    "# Drop the 'price_date' column as year and month are already extracted\n",
    "price_df = price_df.drop(columns=[\"price_date\"]).copy()\n",
    "\n",
    "# Select columns containing numerical data (float64) for processing\n",
    "numeric_cols = price_df.select_dtypes(include=\"Float64\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd655e",
   "metadata": {},
   "source": [
    "#### Difference between prices in December and preceding January\n",
    "\n",
    "To investigate a colleague's idea that the difference in off-peak electricity prices between December and the previous January might predict customer churn, we calculate both the absolute change and the percentage change in all electricity and power prices over the year 2015.\n",
    "\n",
    "##### Absolute 12-month growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "4509f7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_off_peak_energy</th>\n",
       "      <th>price_peak_energy</th>\n",
       "      <th>price_mid_peak_energy</th>\n",
       "      <th>price_off_peak_power</th>\n",
       "      <th>price_peak_power</th>\n",
       "      <th>price_mid_peak_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>038af19179925da21a25619c5a24b745</td>\n",
       "      <td>-0.005508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.177779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>31f2ce549924679a3cbb2d128ae9ea43</td>\n",
       "      <td>-0.007221</td>\n",
       "      <td>-0.002324</td>\n",
       "      <td>0.00356</td>\n",
       "      <td>0.162916</td>\n",
       "      <td>0.097749</td>\n",
       "      <td>0.065166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  price_off_peak_energy  \\\n",
       "11  038af19179925da21a25619c5a24b745              -0.005508   \n",
       "23  31f2ce549924679a3cbb2d128ae9ea43              -0.007221   \n",
       "\n",
       "    price_peak_energy  price_mid_peak_energy  price_off_peak_power  \\\n",
       "11                0.0                    0.0              0.177779   \n",
       "23          -0.002324                0.00356              0.162916   \n",
       "\n",
       "    price_peak_power  price_mid_peak_power  \n",
       "11               0.0                   0.0  \n",
       "23          0.097749              0.065166  "
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the absolute difference in prices for a 12 month period: i.e., December (12) and January (1)\n",
    "price_df_abs_growth_12month = pd.concat([\n",
    "    # Select the 'id' column from the price_df DataFrame\n",
    "    price_df[\"id\"],\n",
    "    # Filter for rows where 'price_month' is in December (12) or January (1)\n",
    "    # and calculate percentage change for 'numeric_cols' grouped by 'id'\n",
    "    price_df[price_df[\"price_month\"].isin([1, 12])].groupby([\"id\"])[numeric_cols].transform(\"diff\").dropna()\n",
    "    ], axis=1).dropna()\n",
    "# output\n",
    "price_df_abs_growth_12month.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0896d",
   "metadata": {},
   "source": [
    "##### Relative 12-month growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "134072c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_off_peak_energy</th>\n",
       "      <th>price_peak_energy</th>\n",
       "      <th>price_mid_peak_energy</th>\n",
       "      <th>price_off_peak_power</th>\n",
       "      <th>price_peak_power</th>\n",
       "      <th>price_mid_peak_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>038af19179925da21a25619c5a24b745</td>\n",
       "      <td>-3.638838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.401606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>31f2ce549924679a3cbb2d128ae9ea43</td>\n",
       "      <td>-5.732044</td>\n",
       "      <td>-2.247691</td>\n",
       "      <td>4.976515</td>\n",
       "      <td>0.401607</td>\n",
       "      <td>0.401607</td>\n",
       "      <td>0.401607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  price_off_peak_energy  \\\n",
       "11  038af19179925da21a25619c5a24b745              -3.638838   \n",
       "23  31f2ce549924679a3cbb2d128ae9ea43              -5.732044   \n",
       "\n",
       "    price_peak_energy  price_mid_peak_energy  price_off_peak_power  \\\n",
       "11                0.0                    0.0              0.401606   \n",
       "23          -2.247691               4.976515              0.401607   \n",
       "\n",
       "    price_peak_power  price_mid_peak_power  \n",
       "11               0.0                   0.0  \n",
       "23          0.401607              0.401607  "
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the percentage change in prices for a 12 month period: i.e., December (12) and January (1)\n",
    "price_df_pct_growth_12month = pd.concat([\n",
    "    price_df[\"id\"],\n",
    "    # Filter for rows where 'price_month' is in December (12) or January (1)\n",
    "    # and calculate percentage change for 'numeric_cols' grouped by 'id'\n",
    "    price_df[price_df[\"price_month\"].isin([1, 12])].groupby([\"id\"])[numeric_cols].transform(\"pct_change\").dropna(),\n",
    "    ], axis=1).dropna()\n",
    "\n",
    "# Fill NaN values in 'price_df_pct_growth_12month' with 0; this denotes (0-0)/0\n",
    "price_df_pct_growth_12month[np.isnan(price_df_pct_growth_12month[numeric_cols])] = 0\n",
    "# Fill +Inf values in 'price_df_pct_growth_12month' with 1; this denotes (positive-0)/0\n",
    "price_df_pct_growth_12month = price_df_pct_growth_12month.replace([np.inf], 1)\n",
    "# Multiply the percentage change by 100 to convert it to a percentage\n",
    "price_df_pct_growth_12month[numeric_cols] = price_df_pct_growth_12month[numeric_cols] * 100\n",
    "# output\n",
    "price_df_pct_growth_12month.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2cb0b2",
   "metadata": {},
   "source": [
    "#### Average monthly growth\n",
    "To further investigate whether price changes are a significant factor in customer churn, we also calculate the average monthly growth rates of customer churn.\n",
    "\n",
    "##### Average absolute monthly growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "a138279b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_off_peak_energy</th>\n",
       "      <th>price_peak_energy</th>\n",
       "      <th>price_mid_peak_energy</th>\n",
       "      <th>price_off_peak_power</th>\n",
       "      <th>price_peak_power</th>\n",
       "      <th>price_mid_peak_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002203ffbb812588b632b9e628cc38d</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>-0.000209</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.014811</td>\n",
       "      <td>0.008886</td>\n",
       "      <td>0.005924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0004351ebdd665e6ee664792efc4fd13</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  price_off_peak_energy  price_peak_energy  \\\n",
       "0  0002203ffbb812588b632b9e628cc38d              -0.000563          -0.000209   \n",
       "1  0004351ebdd665e6ee664792efc4fd13              -0.000373                0.0   \n",
       "\n",
       "   price_mid_peak_energy  price_off_peak_power  price_peak_power  \\\n",
       "0               0.000317              0.014811          0.008886   \n",
       "1                    0.0              0.016162               0.0   \n",
       "\n",
       "   price_mid_peak_power  \n",
       "0              0.005924  \n",
       "1                   0.0  "
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the monthly absolute difference in prices\n",
    "price_df_abs_growth_monthly = pd.concat([\n",
    "    price_df[\"id\"],\n",
    "    # Calculate monthly difference for each numeric column by group 'id'\n",
    "    price_df.groupby([\"id\"])[numeric_cols].transform(\"diff\").dropna()\n",
    "    ], axis=1).dropna()\n",
    "# Calculate average monthly absolute difference by group 'id' and reset index\n",
    "price_df_abs_growth_monthly_avg = price_df_abs_growth_monthly.groupby([\"id\"])[numeric_cols].agg(\"mean\").reset_index()\n",
    "# output\n",
    "price_df_abs_growth_monthly_avg.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaaa37d",
   "metadata": {},
   "source": [
    "##### Average relative monthly growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "b6902df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_off_peak_energy</th>\n",
       "      <th>price_peak_energy</th>\n",
       "      <th>price_mid_peak_energy</th>\n",
       "      <th>price_off_peak_power</th>\n",
       "      <th>price_peak_power</th>\n",
       "      <th>price_mid_peak_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002203ffbb812588b632b9e628cc38d</td>\n",
       "      <td>-0.43736</td>\n",
       "      <td>-0.194842</td>\n",
       "      <td>0.451697</td>\n",
       "      <td>0.03651</td>\n",
       "      <td>0.03651</td>\n",
       "      <td>0.03651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0004351ebdd665e6ee664792efc4fd13</td>\n",
       "      <td>-0.251347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  price_off_peak_energy  price_peak_energy  \\\n",
       "0  0002203ffbb812588b632b9e628cc38d               -0.43736          -0.194842   \n",
       "1  0004351ebdd665e6ee664792efc4fd13              -0.251347                0.0   \n",
       "\n",
       "   price_mid_peak_energy  price_off_peak_power  price_peak_power  \\\n",
       "0               0.451697               0.03651           0.03651   \n",
       "1                    0.0               0.03651               0.0   \n",
       "\n",
       "   price_mid_peak_power  \n",
       "0               0.03651  \n",
       "1                   0.0  "
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the monthly percentage change in prices\n",
    "price_df_pct_growth_monthly = pd.concat([\n",
    "    price_df[\"id\"],\n",
    "    # Calculate the percentage change for each numeric column by group 'id'\n",
    "    price_df.groupby([\"id\"])[numeric_cols].transform(\"pct_change\").dropna()\n",
    "    ], axis=1).dropna()\n",
    "# Replace NaN values with 0 for numeric columns\n",
    "price_df_pct_growth_monthly[np.isnan(price_df_pct_growth_monthly[numeric_cols])] = 0\n",
    "# Fill +Inf values in 'price_df_pct_growth_12month' with 1; this denotes (positive-0)/0\n",
    "price_df_pct_growth_monthly = price_df_pct_growth_monthly.replace([np.inf], 1)\n",
    "# Multiply the percentage change by 100 to convert it to a percentage\n",
    "price_df_pct_growth_monthly[numeric_cols] = price_df_pct_growth_monthly[numeric_cols] * 100\n",
    "# Calculate average monthly percentage change by group 'id' and reset index\n",
    "price_df_pct_growth_monthly_avg = price_df_pct_growth_monthly.groupby([\"id\"])[numeric_cols].agg(\"mean\").reset_index()\n",
    "# output\n",
    "price_df_pct_growth_monthly_avg.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930739ea",
   "metadata": {},
   "source": [
    "#### Joining growth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "f302b440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique user-companies: 16068\n",
      "Rows with NaN:  0\n",
      "Rows with NA:  0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16068 entries, 0 to 16067\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   id                                        16068 non-null  string \n",
      " 1   price_off_peak_energy_absgrowth_12month   16068 non-null  Float64\n",
      " 2   price_peak_energy_absgrowth_12month       16068 non-null  Float64\n",
      " 3   price_mid_peak_energy_absgrowth_12month   16068 non-null  Float64\n",
      " 4   price_off_peak_power_absgrowth_12month    16068 non-null  Float64\n",
      " 5   price_peak_power_absgrowth_12month        16068 non-null  Float64\n",
      " 6   price_mid_peak_power_absgrowth_12month    16068 non-null  Float64\n",
      " 7   price_off_peak_energy_pctgrowth_12month   16068 non-null  Float64\n",
      " 8   price_peak_energy_pctgrowth_12month       16068 non-null  Float64\n",
      " 9   price_mid_peak_energy_pctgrowth_12month   16068 non-null  Float64\n",
      " 10  price_off_peak_power_pctgrowth_12month    16068 non-null  Float64\n",
      " 11  price_peak_power_pctgrowth_12month        16068 non-null  Float64\n",
      " 12  price_mid_peak_power_pctgrowth_12month    16068 non-null  Float64\n",
      " 13  price_off_peak_energy_absgrowth_monthavg  16068 non-null  Float64\n",
      " 14  price_peak_energy_absgrowth_monthavg      16068 non-null  Float64\n",
      " 15  price_mid_peak_energy_absgrowth_monthavg  16068 non-null  Float64\n",
      " 16  price_off_peak_power_absgrowth_monthavg   16068 non-null  Float64\n",
      " 17  price_peak_power_absgrowth_monthavg       16068 non-null  Float64\n",
      " 18  price_mid_peak_power_absgrowth_monthavg   16068 non-null  Float64\n",
      " 19  price_off_peak_energy_pctgrowth_monthavg  16068 non-null  Float64\n",
      " 20  price_peak_energy_pctgrowth_monthavg      16068 non-null  Float64\n",
      " 21  price_mid_peak_energy_pctgrowth_monthavg  16068 non-null  Float64\n",
      " 22  price_off_peak_power_pctgrowth_monthavg   16068 non-null  Float64\n",
      " 23  price_peak_power_pctgrowth_monthavg       16068 non-null  Float64\n",
      " 24  price_mid_peak_power_pctgrowth_monthavg   16068 non-null  Float64\n",
      "dtypes: Float64(24), string(1)\n",
      "memory usage: 3.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Create lists containing dataframes with suffixes added to numeric columns\n",
    "dfs = (\n",
    "    append_suffix(price_df_abs_growth_12month, numeric_cols, \"_absgrowth_12month\"),\n",
    "    append_suffix(price_df_pct_growth_12month, numeric_cols, \"_pctgrowth_12month\"),\n",
    "    append_suffix(price_df_abs_growth_monthly_avg, numeric_cols, \"_absgrowth_monthavg\"),\n",
    "    append_suffix(price_df_pct_growth_monthly_avg, numeric_cols, \"_pctgrowth_monthavg\")\n",
    ")\n",
    "# Initial merge of the first two dataframes in the list\n",
    "data_prices = pd.merge(left=dfs[0], right=dfs[1], on=\"id\")\n",
    "# Loop through remaining dataframes in the list and perform merge\n",
    "for df in dfs[2:]:\n",
    "    data_prices = pd.merge(left=data_prices, right=df, on=\"id\")\n",
    "\n",
    "# unique user-companies\n",
    "print(\"Number of unique user-companies:\", data_prices[\"id\"].nunique())\n",
    "# Count rows with NaN in numeric columns\n",
    "print(\"Rows with NaN: \", len(data_prices[np.isnan(data_prices.select_dtypes(include=[\"Float64\"])).any(axis=1)]))\n",
    "# Count rows with any null value\n",
    "print(\"Rows with NA: \", len(data_prices[data_prices.isnull().any(axis=1)]))\n",
    "# Shape of the final dataframe\n",
    "data_prices.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deccad7",
   "metadata": {},
   "source": [
    "#### Reduce price columns to optimize model's efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2013a7",
   "metadata": {},
   "source": [
    "Our data contained 25 price columns, some of which might be redundant (providing the same information). To improve model efficiency, we addressed this by reducing the number of price variables. \n",
    "\n",
    "Here's how we did it:\n",
    "\n",
    "1. **Identified Collinear Variables:** We first identified which price columns were highly correlated (collinear), meaning they provided very similar information.\n",
    "2. **Extracted Key Components:** We then used a technique called Principal Component Analysis (PCA) to extract the main underlying factors that explained the variation in these collinear variables. \n",
    "3. **Reduced Redundancy:** Finally, we kept these key components (which captured most of the information) and removed the original, redundant price columns from the data. \n",
    "\n",
    "This process resulted in a significant reduction in the number of price columns, from 25 to 9. This not only improves the efficiency of our model but also ensures we retain the important information for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438705d7",
   "metadata": {},
   "source": [
    "**Identify Collinear Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "b64c8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for the numeric columns in 'data_prices'\n",
    "numeric_cols = list(data_prices.select_dtypes(include=[\"Float64\"]).columns)\n",
    "corr_data_prices = correlation_matrix(data_prices, numeric_cols)\n",
    "\n",
    "# Sort the correlation matrix by the 'correlation' column in descending order (highest correlations first)\n",
    "corr_data_prices = corr_data_prices.sort_values([\"correlation\"], ascending=False)\n",
    "\n",
    "# Get descriptive statistics of the correlation matrix to assess its distribution.\n",
    "corr_data_prices.describe(include=[\"Float64\"])\n",
    "\n",
    "# Filter the correlation matrix to only include pairs with correlation >= 0.7 (highly correlated)\n",
    "corr_data_prices_high = corr_data_prices.loc[corr_data_prices[\"correlation\"] >= 0.7]\n",
    "\n",
    "# Extract the column names from both 'name_col1' and 'name_col2' columns in the filtered data frame\n",
    "# Use set union to remove duplicates and get a list of all the highly correlated columns\n",
    "numeric_high_corr_cols = list(set(corr_data_prices_high[\"name_col1\"]).union(corr_data_prices_high[\"name_col2\"]))\n",
    "\n",
    "# columns of colleague's analytical interest\n",
    "cols_colleague = [\"price_off_peak_power_absgrowth_12month\", \"price_off_peak_energy_absgrowth_12month\"]\n",
    "\n",
    "# Filter the highly correlated columns to exclude those of colleague's analytical interest\n",
    "numeric_high_corr_cols_filtered = [col for col in numeric_high_corr_cols if col not in cols_colleague]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa57293",
   "metadata": {},
   "source": [
    "**Extract Key Components and Reduce Redundancy**\n",
    "\n",
    "Descriptive statistics show that the initial max correlation coefficient was reduced to ~0.6 (relative to ~0.9 earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "3b1ffd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16068 entries, 0 to 16067\n",
      "Data columns (total 10 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   id                                   16068 non-null  string \n",
      " 1   price_peak_power_pctgrowth_12month   16068 non-null  Float64\n",
      " 2   price_peak_power_pctgrowth_monthavg  16068 non-null  Float64\n",
      " 3   price_pc_1                           16068 non-null  float64\n",
      " 4   price_pc_2                           16068 non-null  float64\n",
      " 5   price_pc_3                           16068 non-null  float64\n",
      " 6   price_pc_4                           16068 non-null  float64\n",
      " 7   price_pc_5                           16068 non-null  float64\n",
      " 8   colleague_pc_1                       16068 non-null  float64\n",
      " 9   colleague_pc_2                       16068 non-null  float64\n",
      "dtypes: Float64(2), float64(7), string(1)\n",
      "memory usage: 1.3 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.600000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.675618e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.452396e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-8.705119e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.305553e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.158047e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.331128e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.859146e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        correlation\n",
       "count  3.600000e+01\n",
       "mean  -2.675618e-02\n",
       "std    2.452396e-01\n",
       "min   -8.705119e-01\n",
       "25%   -1.305553e-02\n",
       "50%    3.158047e-16\n",
       "75%    1.331128e-02\n",
       "max    5.859146e-01"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract principal components from the data with high correlation columns\n",
    "data_prices_pc_high_corr = extract_principal_components(data_prices, numeric_high_corr_cols_filtered)\n",
    "data_prices_pc_colleague = extract_principal_components(data_prices, cols_colleague)\n",
    "# Drop high correlation columns from the original data\n",
    "data_prices_wpc = data_prices.drop(columns=numeric_high_corr_cols_filtered).copy()\n",
    "# Join the original data with the principal components\n",
    "data_prices_wpc = data_prices_wpc.join(data_prices_pc_high_corr)\n",
    "\n",
    "# Rename columns to facilitate readability\n",
    "data_prices_wpc.columns = [\n",
    "    f\"price_{col}\" if col.startswith(\"pc_\") else col\n",
    "    for col in data_prices_wpc.columns\n",
    "]\n",
    "\n",
    "# Drop colleague columns from the original data\n",
    "data_prices_wpc = data_prices_wpc.drop(columns=cols_colleague).copy()\n",
    "# Join the original data with the principal components\n",
    "data_prices_wpc = data_prices_wpc.join(data_prices_pc_colleague)\n",
    "# Rename columns to facilitate readability\n",
    "data_prices_wpc.columns = [\n",
    "    f\"colleague_{col}\" if col.startswith(\"pc_\") else col\n",
    "    for col in data_prices_wpc.columns\n",
    "]\n",
    "\n",
    "# Calculate correlation matrix for our newly created dataframe\n",
    "numeric_cols = list(data_prices_wpc.select_dtypes(include=[\"Float64\"]).columns)\n",
    "corr_data_prices_wpc = correlation_matrix(data_prices_wpc, numeric_cols)\n",
    "# Sort correlation matrix by correlation in descending order\n",
    "corr_data_prices_wpc = corr_data_prices_wpc.sort_values([\"correlation\"], ascending=False)\n",
    "\n",
    "data_prices_wpc.info()\n",
    "# Descriptive statistics for numeric columns of the correlation matrix\n",
    "corr_data_prices_wpc.describe(include=[\"Float64\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f556b5e",
   "metadata": {},
   "source": [
    "### Removing unclear price features\n",
    "The DataFrame `client_df` contains price data with two prefixes: `forecast_` and `var_`. However, the documentation doesn't clarify the reference date for these values. It's unclear whether:\n",
    "\n",
    "* `forecast_` values are forecasts relative to 2015, or another date like the maximum date in `client_df`.\n",
    "* `var_` prices refer to the year before maximum date in `client_df`, or to a date range within `price_df`.\n",
    "\n",
    "To avoid compromising data integrity and consistency with `price_df`, we choose to remove all columns with these prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "1c42bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2drop = [col for col in client_df.columns if col.startswith((\"forecast_\",\"var_\"))]\n",
    "client_df = client_df.drop(columns=cols2drop).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ffa1b",
   "metadata": {},
   "source": [
    "### User-company characteristics: Datetime features\n",
    "\n",
    "Next, we create new features from the `client_df` data. \n",
    "\n",
    "Since Random Forests only accept numerical inputs, we focus on transforming the datetime data. To prevent introducing randomness (noise) that might negatively impact the model, we create categorical features based on the datetime information. These new features aim to capture the most relevant aspects of the original datetime data.\n",
    "\n",
    "Several datetime columns seem redundant. We consider that `date_activ` captures the same information as `num_years_antiquity`, while `churn` (our target variable) already reflects the information in `date_end`. Additionally, we believe that `date_renewal` does not have a meaningful impact on `churn`. Therefore, we remove `date_activ`, `date_end`, and `date_renewal` from the data.\n",
    "\n",
    "In contrast, we suspect that the date a customer last modified their product (`date_modif_prod`) might influence their likelihood of churning (`churn`). However, our price data only goes back to 2015. To address this shortcoming, we create a new category called `group_modif_prod` to segment customers based on when they modified their product. This category has three values:\n",
    "\n",
    "* **1:**  Customer modified the product before 2015 (controls for previous behavior).\n",
    "* **2:**  Customer modified the product in 2015 (controls price effect).\n",
    "* **3:**  Customer modified the product after 2015 (controls price effect).\n",
    "\n",
    "By grouping customers this way, we can analyze the relationship between product modification date and potential price impacts on churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "d514df8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAs: 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10811 entries, 0 to 14604\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype   \n",
      "---  ------                --------------  -----   \n",
      " 0   id                    10811 non-null  string  \n",
      " 1   channel_sales         10811 non-null  category\n",
      " 2   cons_12m              10811 non-null  Int64   \n",
      " 3   cons_gas_12m          10811 non-null  Int64   \n",
      " 4   cons_last_month       10811 non-null  Int64   \n",
      " 5   has_gas               10811 non-null  bool    \n",
      " 6   imp_cons              10811 non-null  Float64 \n",
      " 7   margin_gross_pow_ele  10811 non-null  Float64 \n",
      " 8   margin_net_pow_ele    10811 non-null  Float64 \n",
      " 9   nb_prod_act           10811 non-null  Int64   \n",
      " 10  net_margin            10811 non-null  Float64 \n",
      " 11  num_years_antig       10811 non-null  Int64   \n",
      " 12  origin_up             10811 non-null  category\n",
      " 13  pow_max               10811 non-null  Float64 \n",
      " 14  churn                 10811 non-null  bool    \n",
      " 15  group_modif_prod      10811 non-null  category\n",
      "dtypes: Float64(5), Int64(5), bool(2), category(3), string(1)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "client_df = client_df.drop(columns=[\"date_activ\",\"date_end\",\"date_renewal\"]).copy()\n",
    "\n",
    "# Define conditions based on the year in \"date_modif_prod\" column\n",
    "condition_list = [\n",
    "    client_df[\"date_modif_prod\"].dt.year < 2015,  # Before 2015\n",
    "    client_df[\"date_modif_prod\"].dt.year == 2015,  # In 2015\n",
    "    client_df[\"date_modif_prod\"].dt.year > 2015   # After 2015\n",
    "]\n",
    "# Define corresponding choices (categories) based on conditions\n",
    "choice_list = [1, 2, 3]\n",
    "# Create a new column \"group_modif_prod\"\n",
    "client_df[\"group_modif_prod\"] = np.select(condition_list, choice_list, default=np.nan)\n",
    "# Convert \"group_modif_prod\" column to categorical type for better handling\n",
    "client_df[\"group_modif_prod\"] = client_df[\"group_modif_prod\"].astype(int).astype(\"category\")\n",
    "\n",
    "# Drop the original \"date_modif_prod\" column (no longer needed)\n",
    "client_df = client_df.drop(columns=[\"date_modif_prod\"]).copy()\n",
    "\n",
    "# Confirm NAs in the dataframe\n",
    "print(\"NAs:\",client_df.isna().sum().sum())\n",
    "client_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6fb6fa",
   "metadata": {},
   "source": [
    "### Join cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "551c319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10791 entries, 0 to 10790\n",
      "Data columns (total 25 columns):\n",
      " #   Column                               Non-Null Count  Dtype   \n",
      "---  ------                               --------------  -----   \n",
      " 0   id                                   10791 non-null  string  \n",
      " 1   channel_sales                        10791 non-null  category\n",
      " 2   cons_12m                             10791 non-null  Int64   \n",
      " 3   cons_gas_12m                         10791 non-null  Int64   \n",
      " 4   cons_last_month                      10791 non-null  Int64   \n",
      " 5   has_gas                              10791 non-null  bool    \n",
      " 6   imp_cons                             10791 non-null  Float64 \n",
      " 7   margin_gross_pow_ele                 10791 non-null  Float64 \n",
      " 8   margin_net_pow_ele                   10791 non-null  Float64 \n",
      " 9   nb_prod_act                          10791 non-null  Int64   \n",
      " 10  net_margin                           10791 non-null  Float64 \n",
      " 11  num_years_antig                      10791 non-null  Int64   \n",
      " 12  origin_up                            10791 non-null  category\n",
      " 13  pow_max                              10791 non-null  Float64 \n",
      " 14  churn                                10791 non-null  bool    \n",
      " 15  group_modif_prod                     10791 non-null  category\n",
      " 16  price_peak_power_pctgrowth_12month   10791 non-null  Float64 \n",
      " 17  price_peak_power_pctgrowth_monthavg  10791 non-null  Float64 \n",
      " 18  price_pc_1                           10791 non-null  float64 \n",
      " 19  price_pc_2                           10791 non-null  float64 \n",
      " 20  price_pc_3                           10791 non-null  float64 \n",
      " 21  price_pc_4                           10791 non-null  float64 \n",
      " 22  price_pc_5                           10791 non-null  float64 \n",
      " 23  colleague_pc_1                       10791 non-null  float64 \n",
      " 24  colleague_pc_2                       10791 non-null  float64 \n",
      "dtypes: Float64(7), Int64(5), bool(2), category(3), float64(7), string(1)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "data_clean = pd.merge(\n",
    "    client_df,\n",
    "    data_prices_wpc,\n",
    "    on=\"id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52378149",
   "metadata": {},
   "source": [
    "## 4. Random Forest ML pre-processing\n",
    "\n",
    "Preparing the data for the Random Forest algorithm depends on the data type:\n",
    "- **Continuous data**: Linear algorithms like linear regression assume a linear relationship between target and explanatory variables, often requiring transformations (e.g., natural logarithms) to linearize relationships. BCG recommends applying these transformations before implementing Random Forest. However, [Raschka & Mirjalili](https://www.google.com/search?q=Raschka+%26+Mirjalili++2017+Python+Machine+Learning) note that \"[...] an advantage of the decision tree algorithm [which form the basis of Random Forest] is that it does not require any transformation of the features if we are dealing with nonlinear data\" (2017: 492). Despite this lack of consensus, we choose to log-transform columns with highly skewed distributions. This allows us to evaluate different models using consistent data preprocessing.\n",
    "- **Categorical data**\n",
    "    - *Boolean*: In Python, boolean values (True and False) are directly equivalent to the integers 0 and 1, respectively. Therefore, there is no need to explicitly convert them.\n",
    "    - *Nominal*: Random forest classifiers implemented with `sklearn.ensemble.RandomForestClassifier` can not handle categorical features directly. To address this shortcoming, we create dummy variables for the categories in each column, where the number of dummy variables for each column is equal to the total number of categories. In turn, we express this as boolean values to improve readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b930dc",
   "metadata": {},
   "source": [
    "#### Continuos data\n",
    "\n",
    "Descriptive statistics reveal that, in several columns, the mean is substantially higher than the median (50th percentile), suggesting right-skewed distributions with long tails. To address this, we apply a logarithmic transformation to the following columns: `cons_12m`, `cons_gas_12m`, `cons_last_month`, and `imp_cons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "4f8eb868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cons_12m</th>\n",
       "      <th>cons_gas_12m</th>\n",
       "      <th>cons_last_month</th>\n",
       "      <th>imp_cons</th>\n",
       "      <th>margin_gross_pow_ele</th>\n",
       "      <th>margin_net_pow_ele</th>\n",
       "      <th>nb_prod_act</th>\n",
       "      <th>net_margin</th>\n",
       "      <th>num_years_antig</th>\n",
       "      <th>pow_max</th>\n",
       "      <th>price_peak_power_pctgrowth_12month</th>\n",
       "      <th>price_peak_power_pctgrowth_monthavg</th>\n",
       "      <th>price_pc_1</th>\n",
       "      <th>price_pc_2</th>\n",
       "      <th>price_pc_3</th>\n",
       "      <th>price_pc_4</th>\n",
       "      <th>price_pc_5</th>\n",
       "      <th>colleague_pc_1</th>\n",
       "      <th>colleague_pc_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>175173.452229</td>\n",
       "      <td>28439.159577</td>\n",
       "      <td>17527.753128</td>\n",
       "      <td>155.041536</td>\n",
       "      <td>25.310845</td>\n",
       "      <td>25.307321</td>\n",
       "      <td>1.276619</td>\n",
       "      <td>195.405969</td>\n",
       "      <td>4.941155</td>\n",
       "      <td>18.009185</td>\n",
       "      <td>-0.303615</td>\n",
       "      <td>-0.028146</td>\n",
       "      <td>-0.006298</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>-0.026256</td>\n",
       "      <td>-0.022913</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>-0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>613178.368323</td>\n",
       "      <td>170127.857129</td>\n",
       "      <td>68261.767156</td>\n",
       "      <td>318.317652</td>\n",
       "      <td>21.045083</td>\n",
       "      <td>21.044047</td>\n",
       "      <td>0.680272</td>\n",
       "      <td>340.740333</td>\n",
       "      <td>1.218624</td>\n",
       "      <td>12.839611</td>\n",
       "      <td>8.986255</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>3.298342</td>\n",
       "      <td>1.801486</td>\n",
       "      <td>1.576722</td>\n",
       "      <td>0.208815</td>\n",
       "      <td>0.862353</td>\n",
       "      <td>1.210975</td>\n",
       "      <td>0.672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-11.111111</td>\n",
       "      <td>-42.790901</td>\n",
       "      <td>-43.972355</td>\n",
       "      <td>-33.869749</td>\n",
       "      <td>-8.332321</td>\n",
       "      <td>-25.149200</td>\n",
       "      <td>-14.636989</td>\n",
       "      <td>-20.094679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5896.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.64</td>\n",
       "      <td>14.64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.655</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018627</td>\n",
       "      <td>-0.435714</td>\n",
       "      <td>-0.197227</td>\n",
       "      <td>-0.036381</td>\n",
       "      <td>-0.160726</td>\n",
       "      <td>-0.004366</td>\n",
       "      <td>-0.106039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14979.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>42.45</td>\n",
       "      <td>22.21</td>\n",
       "      <td>22.21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>116.89</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>-0.143568</td>\n",
       "      <td>-0.179320</td>\n",
       "      <td>-0.014076</td>\n",
       "      <td>-0.034281</td>\n",
       "      <td>0.132649</td>\n",
       "      <td>0.003884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44206.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3632.5</td>\n",
       "      <td>197.495</td>\n",
       "      <td>30.22</td>\n",
       "      <td>30.22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>252.195</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.05</td>\n",
       "      <td>0.401607</td>\n",
       "      <td>0.03651</td>\n",
       "      <td>0.194180</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.372364</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>0.097530</td>\n",
       "      <td>0.272776</td>\n",
       "      <td>0.144767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6207104.0</td>\n",
       "      <td>4154590.0</td>\n",
       "      <td>771203.0</td>\n",
       "      <td>9682.89</td>\n",
       "      <td>314.76</td>\n",
       "      <td>314.76</td>\n",
       "      <td>32.0</td>\n",
       "      <td>24570.65</td>\n",
       "      <td>11.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>43.586055</td>\n",
       "      <td>53.430582</td>\n",
       "      <td>38.308531</td>\n",
       "      <td>3.570205</td>\n",
       "      <td>32.447748</td>\n",
       "      <td>27.236570</td>\n",
       "      <td>21.844747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cons_12m   cons_gas_12m  cons_last_month    imp_cons  \\\n",
       "count        10791.0        10791.0          10791.0     10791.0   \n",
       "mean   175173.452229   28439.159577     17527.753128  155.041536   \n",
       "std    613178.368323  170127.857129     68261.767156  318.317652   \n",
       "min              0.0            0.0              0.0         0.0   \n",
       "25%           5896.5            0.0              0.0         0.0   \n",
       "50%          14979.0            0.0            847.0       42.45   \n",
       "75%          44206.0            0.0           3632.5     197.495   \n",
       "max        6207104.0      4154590.0         771203.0     9682.89   \n",
       "\n",
       "       margin_gross_pow_ele  margin_net_pow_ele  nb_prod_act  net_margin  \\\n",
       "count               10791.0             10791.0      10791.0     10791.0   \n",
       "mean              25.310845           25.307321     1.276619  195.405969   \n",
       "std               21.045083           21.044047     0.680272  340.740333   \n",
       "min                     0.0                 0.0          1.0         0.0   \n",
       "25%                   14.64               14.64          1.0      52.655   \n",
       "50%                   22.21               22.21          1.0      116.89   \n",
       "75%                   30.22               30.22          1.0     252.195   \n",
       "max                  314.76              314.76         32.0    24570.65   \n",
       "\n",
       "       num_years_antig    pow_max  price_peak_power_pctgrowth_12month  \\\n",
       "count          10791.0    10791.0                             10791.0   \n",
       "mean          4.941155  18.009185                           -0.303615   \n",
       "std           1.218624  12.839611                            8.986255   \n",
       "min                2.0        3.3                              -100.0   \n",
       "25%                4.0       13.0                                 0.0   \n",
       "50%                5.0     13.856                                 0.0   \n",
       "75%                6.0      19.05                            0.401607   \n",
       "max               11.0      320.0                               100.0   \n",
       "\n",
       "       price_peak_power_pctgrowth_monthavg    price_pc_1    price_pc_2  \\\n",
       "count                              10791.0  10791.000000  10791.000000   \n",
       "mean                             -0.028146     -0.006298      0.000865   \n",
       "std                               0.819987      3.298342      1.801486   \n",
       "min                             -11.111111    -42.790901    -43.972355   \n",
       "25%                                    0.0      0.018627     -0.435714   \n",
       "50%                                    0.0      0.043651     -0.143568   \n",
       "75%                                0.03651      0.194180      0.000662   \n",
       "max                               9.999999     43.586055     53.430582   \n",
       "\n",
       "         price_pc_3    price_pc_4    price_pc_5  colleague_pc_1  \\\n",
       "count  10791.000000  10791.000000  10791.000000    10791.000000   \n",
       "mean      -0.026256     -0.022913      0.009072        0.003772   \n",
       "std        1.576722      0.208815      0.862353        1.210975   \n",
       "min      -33.869749     -8.332321    -25.149200      -14.636989   \n",
       "25%       -0.197227     -0.036381     -0.160726       -0.004366   \n",
       "50%       -0.179320     -0.014076     -0.034281        0.132649   \n",
       "75%        0.372364      0.011215      0.097530        0.272776   \n",
       "max       38.308531      3.570205     32.447748       27.236570   \n",
       "\n",
       "       colleague_pc_2  \n",
       "count    10791.000000  \n",
       "mean        -0.002968  \n",
       "std          0.672200  \n",
       "min        -20.094679  \n",
       "25%         -0.106039  \n",
       "50%          0.003884  \n",
       "75%          0.144767  \n",
       "max         21.844747  "
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.describe(include=[\"Int64\",\"Float64\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16beeee2",
   "metadata": {},
   "source": [
    "Log-transformation brings the mean and median closer together. This reduces skewness in the data, which helps satisfy the assumption of linearity between the target and explanatory variables common in methods, such as linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "49fc6d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cons_12m</th>\n",
       "      <th>cons_gas_12m</th>\n",
       "      <th>cons_last_month</th>\n",
       "      <th>imp_cons</th>\n",
       "      <th>margin_gross_pow_ele</th>\n",
       "      <th>margin_net_pow_ele</th>\n",
       "      <th>nb_prod_act</th>\n",
       "      <th>net_margin</th>\n",
       "      <th>num_years_antig</th>\n",
       "      <th>pow_max</th>\n",
       "      <th>price_peak_power_pctgrowth_12month</th>\n",
       "      <th>price_peak_power_pctgrowth_monthavg</th>\n",
       "      <th>price_pc_1</th>\n",
       "      <th>price_pc_2</th>\n",
       "      <th>price_pc_3</th>\n",
       "      <th>price_pc_4</th>\n",
       "      <th>price_pc_5</th>\n",
       "      <th>colleague_pc_1</th>\n",
       "      <th>colleague_pc_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.0</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "      <td>10791.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.828444</td>\n",
       "      <td>1.716915</td>\n",
       "      <td>5.355709</td>\n",
       "      <td>2.927514</td>\n",
       "      <td>25.310845</td>\n",
       "      <td>25.307321</td>\n",
       "      <td>1.276619</td>\n",
       "      <td>195.405969</td>\n",
       "      <td>4.941155</td>\n",
       "      <td>18.009185</td>\n",
       "      <td>-0.303615</td>\n",
       "      <td>-0.028146</td>\n",
       "      <td>-0.006298</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>-0.026256</td>\n",
       "      <td>-0.022913</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.003772</td>\n",
       "      <td>-0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.998298</td>\n",
       "      <td>3.893507</td>\n",
       "      <td>4.075058</td>\n",
       "      <td>2.611064</td>\n",
       "      <td>21.045083</td>\n",
       "      <td>21.044047</td>\n",
       "      <td>0.680272</td>\n",
       "      <td>340.740333</td>\n",
       "      <td>1.218624</td>\n",
       "      <td>12.839611</td>\n",
       "      <td>8.986255</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>3.298342</td>\n",
       "      <td>1.801486</td>\n",
       "      <td>1.576722</td>\n",
       "      <td>0.208815</td>\n",
       "      <td>0.862353</td>\n",
       "      <td>1.210975</td>\n",
       "      <td>0.672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-11.111111</td>\n",
       "      <td>-42.790901</td>\n",
       "      <td>-43.972355</td>\n",
       "      <td>-33.869749</td>\n",
       "      <td>-8.332321</td>\n",
       "      <td>-25.149200</td>\n",
       "      <td>-14.636989</td>\n",
       "      <td>-20.094679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.682284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.64</td>\n",
       "      <td>14.64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.655</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018627</td>\n",
       "      <td>-0.435714</td>\n",
       "      <td>-0.197227</td>\n",
       "      <td>-0.036381</td>\n",
       "      <td>-0.160726</td>\n",
       "      <td>-0.004366</td>\n",
       "      <td>-0.106039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.614471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.742881</td>\n",
       "      <td>3.771611</td>\n",
       "      <td>22.21</td>\n",
       "      <td>22.21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>116.89</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>-0.143568</td>\n",
       "      <td>-0.179320</td>\n",
       "      <td>-0.014076</td>\n",
       "      <td>-0.034281</td>\n",
       "      <td>0.132649</td>\n",
       "      <td>0.003884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.696638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.197952</td>\n",
       "      <td>5.290764</td>\n",
       "      <td>30.22</td>\n",
       "      <td>30.22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>252.195</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.05</td>\n",
       "      <td>0.401607</td>\n",
       "      <td>0.03651</td>\n",
       "      <td>0.194180</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.372364</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>0.097530</td>\n",
       "      <td>0.272776</td>\n",
       "      <td>0.144767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.641205</td>\n",
       "      <td>15.239725</td>\n",
       "      <td>13.555708</td>\n",
       "      <td>9.178219</td>\n",
       "      <td>314.76</td>\n",
       "      <td>314.76</td>\n",
       "      <td>32.0</td>\n",
       "      <td>24570.65</td>\n",
       "      <td>11.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>43.586055</td>\n",
       "      <td>53.430582</td>\n",
       "      <td>38.308531</td>\n",
       "      <td>3.570205</td>\n",
       "      <td>32.447748</td>\n",
       "      <td>27.236570</td>\n",
       "      <td>21.844747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cons_12m  cons_gas_12m  cons_last_month  imp_cons  \\\n",
       "count    10791.0       10791.0          10791.0   10791.0   \n",
       "mean    9.828444      1.716915         5.355709  2.927514   \n",
       "std     1.998298      3.893507         4.075058  2.611064   \n",
       "min          0.0           0.0              0.0       0.0   \n",
       "25%     8.682284           0.0              0.0       0.0   \n",
       "50%     9.614471           0.0         6.742881  3.771611   \n",
       "75%    10.696638           0.0         8.197952  5.290764   \n",
       "max    15.641205     15.239725        13.555708  9.178219   \n",
       "\n",
       "       margin_gross_pow_ele  margin_net_pow_ele  nb_prod_act  net_margin  \\\n",
       "count               10791.0             10791.0      10791.0     10791.0   \n",
       "mean              25.310845           25.307321     1.276619  195.405969   \n",
       "std               21.045083           21.044047     0.680272  340.740333   \n",
       "min                     0.0                 0.0          1.0         0.0   \n",
       "25%                   14.64               14.64          1.0      52.655   \n",
       "50%                   22.21               22.21          1.0      116.89   \n",
       "75%                   30.22               30.22          1.0     252.195   \n",
       "max                  314.76              314.76         32.0    24570.65   \n",
       "\n",
       "       num_years_antig    pow_max  price_peak_power_pctgrowth_12month  \\\n",
       "count          10791.0    10791.0                             10791.0   \n",
       "mean          4.941155  18.009185                           -0.303615   \n",
       "std           1.218624  12.839611                            8.986255   \n",
       "min                2.0        3.3                              -100.0   \n",
       "25%                4.0       13.0                                 0.0   \n",
       "50%                5.0     13.856                                 0.0   \n",
       "75%                6.0      19.05                            0.401607   \n",
       "max               11.0      320.0                               100.0   \n",
       "\n",
       "       price_peak_power_pctgrowth_monthavg    price_pc_1    price_pc_2  \\\n",
       "count                              10791.0  10791.000000  10791.000000   \n",
       "mean                             -0.028146     -0.006298      0.000865   \n",
       "std                               0.819987      3.298342      1.801486   \n",
       "min                             -11.111111    -42.790901    -43.972355   \n",
       "25%                                    0.0      0.018627     -0.435714   \n",
       "50%                                    0.0      0.043651     -0.143568   \n",
       "75%                                0.03651      0.194180      0.000662   \n",
       "max                               9.999999     43.586055     53.430582   \n",
       "\n",
       "         price_pc_3    price_pc_4    price_pc_5  colleague_pc_1  \\\n",
       "count  10791.000000  10791.000000  10791.000000    10791.000000   \n",
       "mean      -0.026256     -0.022913      0.009072        0.003772   \n",
       "std        1.576722      0.208815      0.862353        1.210975   \n",
       "min      -33.869749     -8.332321    -25.149200      -14.636989   \n",
       "25%       -0.197227     -0.036381     -0.160726       -0.004366   \n",
       "50%       -0.179320     -0.014076     -0.034281        0.132649   \n",
       "75%        0.372364      0.011215      0.097530        0.272776   \n",
       "max       38.308531      3.570205     32.447748       27.236570   \n",
       "\n",
       "       colleague_pc_2  \n",
       "count    10791.000000  \n",
       "mean        -0.002968  \n",
       "std          0.672200  \n",
       "min        -20.094679  \n",
       "25%         -0.106039  \n",
       "50%          0.003884  \n",
       "75%          0.144767  \n",
       "max         21.844747  "
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_2log = [\"cons_12m\",\"cons_gas_12m\",\"cons_last_month\",\"imp_cons\"]\n",
    "data_clean[cols_2log] = data_clean[cols_2log] + 1\n",
    "data_clean[cols_2log] = np.log(data_clean[cols_2log])\n",
    "data_clean.describe(include=[\"Int64\",\"Float64\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49744685",
   "metadata": {},
   "source": [
    "#### Continuos data: Nominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "2297bd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boolean           11\n",
       "Float64           10\n",
       "float64            7\n",
       "bool               5\n",
       "Int64              2\n",
       "string[python]     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_nominal = data_clean.select_dtypes(include=[\"category\"]).columns.tolist()\n",
    "for col in cols_nominal:\n",
    "    col_encoded = pd.get_dummies(data_clean[col], prefix=col)\n",
    "    data_clean = data_clean.drop(columns=col).copy()\n",
    "    data_clean = pd.concat([data_clean, col_encoded], axis = 1)\n",
    "data_clean.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99bc69",
   "metadata": {},
   "source": [
    "### Final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "81c9329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10791 entries, 0 to 10790\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   id                                              10791 non-null  string \n",
      " 1   cons_12m                                        10791 non-null  Float64\n",
      " 2   cons_gas_12m                                    10791 non-null  Float64\n",
      " 3   cons_last_month                                 10791 non-null  Float64\n",
      " 4   has_gas                                         10791 non-null  bool   \n",
      " 5   imp_cons                                        10791 non-null  Float64\n",
      " 6   margin_gross_pow_ele                            10791 non-null  Float64\n",
      " 7   margin_net_pow_ele                              10791 non-null  Float64\n",
      " 8   nb_prod_act                                     10791 non-null  Int64  \n",
      " 9   net_margin                                      10791 non-null  Float64\n",
      " 10  num_years_antig                                 10791 non-null  Int64  \n",
      " 11  pow_max                                         10791 non-null  Float64\n",
      " 12  churn                                           10791 non-null  bool   \n",
      " 13  price_peak_power_pctgrowth_12month              10791 non-null  Float64\n",
      " 14  price_peak_power_pctgrowth_monthavg             10791 non-null  Float64\n",
      " 15  price_pc_1                                      10791 non-null  float64\n",
      " 16  price_pc_2                                      10791 non-null  float64\n",
      " 17  price_pc_3                                      10791 non-null  float64\n",
      " 18  price_pc_4                                      10791 non-null  float64\n",
      " 19  price_pc_5                                      10791 non-null  float64\n",
      " 20  colleague_pc_1                                  10791 non-null  float64\n",
      " 21  colleague_pc_2                                  10791 non-null  float64\n",
      " 22  channel_sales_epumfxlbckeskwekxbiuasklxalciiuu  10791 non-null  boolean\n",
      " 23  channel_sales_ewpakwlliwisiwduibdlfmalxowmwpci  10791 non-null  boolean\n",
      " 24  channel_sales_fixdbufsefwooaasfcxdxadsiekoceaa  10791 non-null  boolean\n",
      " 25  channel_sales_foosdfpfkusacimwkcsosbicdxkicaua  10791 non-null  boolean\n",
      " 26  channel_sales_lmkebamcaaclubfxadlmueccxoimlema  10791 non-null  boolean\n",
      " 27  channel_sales_sddiedcslfslkckwlfkdpoeeailfpeds  10791 non-null  boolean\n",
      " 28  channel_sales_usilxuppasemubllopkaafesmlibmsdf  10791 non-null  boolean\n",
      " 29  origin_up_kamkkxfxxuwbdslkwifmmcsiusiuosws      10791 non-null  boolean\n",
      " 30  origin_up_ldkssxwpmemidmecebumciepifcamkci      10791 non-null  boolean\n",
      " 31  origin_up_lxidpiddsbxsbosboudacockeimpuepw      10791 non-null  boolean\n",
      " 32  origin_up_usapbepcfoloekilkwsdiboslwaxobdp      10791 non-null  boolean\n",
      " 33  group_modif_prod_1                              10791 non-null  bool   \n",
      " 34  group_modif_prod_2                              10791 non-null  bool   \n",
      " 35  group_modif_prod_3                              10791 non-null  bool   \n",
      "dtypes: Float64(10), Int64(2), bool(5), boolean(11), float64(7), string(1)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "data_clean.info()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "152bf6e7dc8ee53edb5af21dc1a8faeab7f134840808a94079ed98d91ece7e0c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
